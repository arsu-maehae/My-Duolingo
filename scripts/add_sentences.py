#!/usr/bin/env python3
"""
‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (jp_sentence & th_sentence) ‡∏•‡∏á‡πÉ‡∏ô CSV ‡∏î‡πâ‡∏ß‡∏¢ Gemini (Batch Processing, quota-friendly)

- ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•: gemini-2.5-flash-lite (default)
- ‡∏Ñ‡∏∏‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô requests/day ‡∏î‡πâ‡∏ß‡∏¢ batch_size ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠
- parse JSON ‡πÅ‡∏ö‡∏ö robust
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö atomic + checkpoint
- ‡∏ñ‡πâ‡∏≤‡∏ä‡∏ô daily free-tier quota -> ‡πÄ‡∏ã‡∏ü‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
"""

import argparse
import csv
import json
import logging
import os
import random
import re
import sys
import time
from pathlib import Path

from google import genai

DEFAULT_FILES = ["n5.csv"]


def get_gemini_key() -> str:
    key = os.environ.get("GEMINI_API_KEY")
    if not key or not key.strip():
        logging.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ô (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á)")
        sys.exit(2)
    return key.strip()


def extract_json_object(text: str) -> dict:
    raw = (text or "").strip()

    # Strip markdown fences if any
    if raw.startswith("```"):
        raw = re.sub(r"^```[a-zA-Z]*\s*", "", raw)
        raw = re.sub(r"\s*```$", "", raw).strip()

    # Try direct parse
    try:
        obj = json.loads(raw)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Find a JSON object substring
    m = re.search(r"\{[\s\S]*\}", raw)
    if not m:
        raise ValueError("No JSON object found in model output")

    candidate = m.group(0).strip()
    obj = json.loads(candidate)
    if not isinstance(obj, dict):
        raise ValueError("Extracted JSON is not an object")
    return obj


def is_quota_daily_free_tier(exc: Exception) -> bool:
    msg = str(exc)
    return (
        "generate_content_free_tier_requests" in msg
        and "GenerateRequestsPerDayPerProjectPerModel-FreeTier" in msg
    )


def is_rate_limit(exc: Exception) -> bool:
    msg = str(exc)
    return (
        "429" in msg
        or "RESOURCE_EXHAUSTED" in msg
        or "Too Many Requests" in msg
        or "rate" in msg.lower()
        or "quota" in msg.lower()
    )


def write_csv_atomic(path: Path, fieldnames, rows) -> None:
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    with tmp_path.open("w", encoding="utf-8-sig", newline="") as fh:
        writer = csv.DictWriter(fh, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    tmp_path.replace(path)


def generate_sentences_batch(client, batch_rows, model_name: str, max_retries: int = 3) -> dict:
    """
    Return dict:
      { "0": {"jp": "...", "th": "..."}, "5": {"jp": "...", "th": "..."} }
    where keys are local indices inside batch_rows.
    """

    items = []
    for i, row in enumerate(batch_rows):
        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
        if (row.get("jp_sentence") or "").strip() and (row.get("th_sentence") or "").strip():
            continue

        jp_word = (row.get("expression") or "").strip()
        meaning_th = (row.get("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢") or "").strip()
        reading = (row.get("reading") or "").strip()

        if not jp_word:
            continue

        items.append((i, jp_word, reading, meaning_th))

    if not items:
        return {}

    # Prompt ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ä‡∏±‡∏î ‡πÜ (‡∏•‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏´‡∏•‡∏∏‡∏î JSON)
    items_text = "\n".join(
        f'{idx}: word="{jp_word}" reading="{reading}" th_hint="{meaning_th}"'
        for (idx, jp_word, reading, meaning_th) in items
    )

    prompt = f"""You are a Japanese teacher.

For EACH item, create ONE short, natural example sentence for JLPT N5 learners:
- Must include the given "word"
- Keep sentence short and simple
- Provide Thai translation
- Avoid rare kanji and long grammar
- Output STRICT JSON only:
{{ "ID": {{"jp":"...","th":"..."}}, ... }}
No extra text.

Items:
{items_text}
"""

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = client.models.generate_content(
                model=model_name,
                contents=prompt,
            )
            parsed = extract_json_object((resp.text or "").strip())
            return parsed

        except Exception as exc:
            last_err = exc

            # ‡∏ñ‡πâ‡∏≤‡∏ä‡∏ô daily free-tier: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retry
            if is_quota_daily_free_tier(exc):
                raise

            if is_rate_limit(exc):
                # backoff ‡∏™‡∏±‡πâ‡∏ô ‡πÜ (‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô daily limit ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡πâ‡∏ß)
                sleep_s = min(120.0, 10.0 * (2 ** (attempt - 1)))
                sleep_s *= random.uniform(0.9, 1.2)
                logging.warning(f"‚ö†Ô∏è 429/Quota (attempt {attempt}/{max_retries}) -> sleep {sleep_s:.1f}s")
                time.sleep(sleep_s)
            else:
                sleep_s = min(20.0, 3.0 * attempt)
                logging.warning(f"‚ùå Error (attempt {attempt}/{max_retries}): {exc} -> sleep {sleep_s:.1f}s")
                time.sleep(sleep_s)

    logging.error(f"‚ùå Failed after retries. Last error: {last_err}")
    return {}


def process_file(
    client,
    path: Path,
    batch_size: int,
    model_name: str,
    checkpoint_every: int,
    min_interval: float,
) -> None:
    logging.info(f"üìÇ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå: {path.name}")
    if not path.exists():
        logging.warning(f"‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠: {path}")
        return

    with path.open("r", encoding="utf-8-sig", newline="") as fh:
        reader = csv.DictReader(fh)
        rows = list(reader)
        fieldnames = reader.fieldnames or []

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if "jp_sentence" not in fieldnames:
        fieldnames.append("jp_sentence")
    if "th_sentence" not in fieldnames:
        fieldnames.append("th_sentence")

    total = len(rows)
    updated = 0
    api_calls = 0
    batches_since_ckpt = 0
    last_call_ts = 0.0

    def pace():
        nonlocal last_call_ts
        now = time.time()
        wait = (last_call_ts + min_interval) - now
        if wait > 0:
            time.sleep(wait)
        last_call_ts = time.time()

    for start in range(0, total, batch_size):
        batch = rows[start:start + batch_size]

        # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô batch ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°
        if all((r.get("jp_sentence") or "").strip() and (r.get("th_sentence") or "").strip() for r in batch):
            continue

        logging.info(f"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà {start+1} ‡∏ñ‡∏∂‡∏á {min(start+batch_size, total)} / {total}")

        try:
            pace()
            results = generate_sentences_batch(client, batch, model_name=model_name)
            api_calls += 1
        except Exception as exc:
            if is_quota_daily_free_tier(exc):
                logging.error("üõë ‡∏ä‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Free tier ‡πÅ‡∏•‡πâ‡∏ß (requests/day/model). ‡πÄ‡∏ã‡∏ü‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ.")
                write_csv_atomic(path, fieldnames, rows)
                return
            raise

        changed_this_batch = 0
        for idx_str, data in (results or {}).items():
            try:
                idx = int(idx_str)
            except Exception:
                continue
            if idx < 0 or idx >= len(batch):
                continue
            if not isinstance(data, dict):
                continue

            jp = (data.get("jp") or "").strip()
            th = (data.get("th") or "").strip()
            if not jp or not th:
                continue

            if not (batch[idx].get("jp_sentence") or "").strip():
                batch[idx]["jp_sentence"] = jp
            if not (batch[idx].get("th_sentence") or "").strip():
                batch[idx]["th_sentence"] = th

            updated += 1
            changed_this_batch += 1

        logging.info(f"   ‚Ü≥ ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {changed_this_batch} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏ô batch ‡∏ô‡∏µ‡πâ")

        batches_since_ckpt += 1
        if batches_since_ckpt >= checkpoint_every:
            write_csv_atomic(path, fieldnames, rows)
            logging.info(f"üíæ checkpoint: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß (‡∏ó‡∏∏‡∏Å {checkpoint_every} batch)")
            batches_since_ckpt = 0

    write_csv_atomic(path, fieldnames, rows)
    logging.info(f"‚úÖ ‡πÑ‡∏ü‡∏•‡πå {path.name} ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {updated} ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ")
    logging.info(f"üì° API calls ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: {api_calls}\n")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--files", nargs="*", default=DEFAULT_FILES)

    # quota-friendly defaults (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ free tier ‡∏à‡∏≥‡∏Å‡∏±‡∏î request/day)
    parser.add_argument("--batch-size", type=int, default=150)
    parser.add_argument("--model", type=str, default="gemini-2.5-flash-lite")

    parser.add_argument("--checkpoint-every", type=int, default=2)
    parser.add_argument("--min-interval", type=float, default=3.0)

    args = parser.parse_args()
    logging.basicConfig(level=logging.INFO, format="%(message)s")

    api_key = get_gemini_key()
    client = genai.Client(api_key=api_key)

    data_dir = Path(__file__).resolve().parent.parent / "jp_datasets"

    for fname in args.files:
        fpath = data_dir / fname
        process_file(
            client=client,
            path=fpath,
            batch_size=args.batch_size,
            model_name=args.model,
            checkpoint_every=args.checkpoint_every,
            min_interval=args.min_interval,
        )


if __name__ == "__main__":
    main()